





import numpy as np
import matplotlib.pyplot as plt
from scipy import optimize
data_filename = 'Example2_Data.tsv'
    # This defines the name of our data file.

using_colab = False
    # Change to False if you are running Juptyer locally.





if using_colab:
  from google.colab import files
  uploaded = files.upload()
  data_filename = next(iter(uploaded.keys()))
  # This last line is a trick that will automatically pull the filename from what you uploaded.





print(open(data_filename).read())





x, y, dy = np.loadtxt(data_filename, unpack=True, skiprows=1, usecols=[0,1,2])





print(x)





%matplotlib inline
fig, ax = plt.subplots(figsize = (10,8))
ax.errorbar(x,y,dy)





def linear(p, xvar):
    return p[0] + p[1]*xvar





print(x)
print(x[0])
print(x[2])
print(x[-2])





fig, ax = plt.subplots(figsize = (10,8))
ax.errorbar(x,y,dy,fmt='k.',capsize = 3)
guess = [0,3]
calculated_y = linear(guess,x)
ax.plot(x,calculated_y)





fig, ax = plt.subplots(figsize = (10,8))
ax.errorbar(x,y,dy,fmt='k.',capsize = 3)
my_guess = [1,3.5]
calculated_y = linear(my_guess,x)
ax.plot(x,calculated_y)





def residual(p,func, xvar, yvar, err):
    return (func(p, xvar) - yvar)/err
    #Yes, this is a function of a function.  You can do that!





print(residual(guess,linear,x,y,dy))





chisq = sum(residual(guess,linear,x,y,dy)**2)
print(chisq)





fig = plt.figure(figsize = (10,8))
ax =fig.add_subplot(2,1,1)
ay =fig.add_subplot(2,1,2)
#fig,(ax,ay) = plt.subplots(nrows=2, ncols=1)
#the above is an equivalent way of doing the same thing

ax.errorbar(x,y,dy,fmt='k.',label='Data',capsize=3)
    # A basic scatter plot of some data in red
ax.plot(x,linear(guess,x),label='Fit')
    # A plot of just the magnitudes of the errors
    # Note that you don't have to calculate the y values ahead of time, you can just use linear when calling the plot function.
ax.set_title("Data")
ax.legend()

ay.plot(x,residual(guess,linear, x, y, dy),'r.',label='Residuals')
ay.hlines(0,0,10)
ay.set_title("Residuals")
ay.legend()
fig.tight_layout()

chisq = sum(residual(guess,linear,x,y,dy)**2)
print("chisq value =",chisq)








fig, (ax0, ax1) = plt.subplots(2, 1, figsize=(10, 8))

ax0.errorbar(x,y,dy,fmt='k.',label='Data',capsize=3)
ax0.plot(x,linear(guess,x),label='Fit')
ax0.plot(x, linear(my_guess, x), label='My Fit')
    # A plot of just the magnitudes of the errors
    # Note that you don't have to calculate the y values ahead of time, you can just use linear when calling the plot function.
ax0.set_title("Data")
ax0.legend()

ax1.plot(x,residual(guess,linear, x, y, dy),'r.',label='Residuals')
ax1.plot(x, residual(my_guess, linear, x, y, dy), 'g.', label='My Residuals')
ax1.hlines(0,0,10)
ax1.set_title("Residuals")
ax1.legend()
fig.tight_layout()

chisq = sum(residual(my_guess,linear,x,y,dy)**2)
print("chisq value =",chisq)





animal = "frogs"
dumb_animal = "blobfish"
number = 90.01
print("I think that {} are {:.0%} cooler than {}.".format(animal,number,dumb_animal))





chisq = sum(residual(guess,linear,x,y,dy)**2)
print("chisq value = {:0.2f}".format(chisq))





variable_1, dvariable_1 = 10, 0.412

print('$a = {:.2f} \\pm {:.2f}$'.format(variable_1,dvariable_1))

from IPython.display import display, Latex 
    #Python doesn't know how to render math with the print command, but we can make it do so with display(Latex())
display(Latex('$a = {:.2f} \\pm {:.2f}$'.format(variable_1,dvariable_1)))





a = 2.28
aerr = 1

display(Latex('$a = {:.2} \\pm {:.2}$ \n'.format(a,aerr)))





a = 2.28
aerr = 1

display(Latex('$a = {:.2f} \\pm {:.2f}$ \n'.format(a,aerr)))





optimize.least_squares(residual, guess, args=(linear,x, y, dy))





# The code below defines our data fitting function.
# Inputs are:
# initial guess for parameters p0
# the function we're fitting to
# the x,y, and dy variables
# tmi can be set to 1 or 2 if more intermediate data is needed

def data_fit(p0,func,xvar, yvar, err,tmi=0):
    try:
        fit = optimize.least_squares(residual, p0, args=(func,xvar, yvar, err),verbose=tmi)
    except Exception as error:
        print("Something has gone wrong:",error)
        return p0,np.zeros_like(p0),np.nan,np.nan
    pf = fit['x']

    print()

    try:
        cov = np.linalg.inv(fit['jac'].T.dot(fit['jac']))          
        # This computes a covariance matrix by finding the inverse of the Jacobian times its transpose
        # We need this to find the uncertainty in our fit parameters
    except:
        # If the fit failed, print the reason
        print('Fit did not converge')
        print('Result is likely a local minimum')
        print('Try changing initial values')
        print('Status code:', fit['status'])
        print(fit['message'])
        return pf,np.zeros_like(pf),np.nan,np.nan
            #You'll be able to plot with this, but it will not be a good fit.

    chisq = sum(residual(pf,func,xvar, yvar, err) **2)
    dof = len(xvar) - len(pf)
    red_chisq = chisq/dof
    pferr = np.sqrt(np.diagonal(cov)) # finds the uncertainty in fit parameters by squaring diagonal elements of the covariance matrix
    print('Converged with chi-squared {:.2f}'.format(chisq))
    print('Number of degrees of freedom, dof = {:.2f}'.format(dof))
    print('Reduced chi-squared {:.2f}'.format(red_chisq))
    print()
    Columns = ["Parameter #","Initial guess values:", "Best fit values:", "Uncertainties in the best fit values:"]
    print('{:<11}'.format(Columns[0]),'|','{:<24}'.format(Columns[1]),"|",'{:<24}'.format(Columns[2]),"|",'{:<24}'.format(Columns[3]))
    for num in range(len(pf)):
        print('{:<11}'.format(num),'|','{:<24.3e}'.format(p0[num]),'|','{:<24.3e}'.format(pf[num]),'|','{:<24.3e}'.format(pferr[num]))
    return pf, pferr, chisq,dof





print("Linear Fit")
pf, pferr, chisq, dof = data_fit(guess,linear, x, y, dy)





print("Linear Fit")
my_pf, my_pferr, my_chisq, my_dof = data_fit(my_guess,linear, x, y, dy)





fig = plt.figure(figsize = (10,8))
ax = fig.add_subplot(1,1,1)
ax.errorbar(x, y, dy, fmt='ko', label = 'Data')
X = np.linspace(x.min(), x.max(), 500)
ax.plot(X, linear(pf, X), 'r-', label = 'Linear Fit: $f(x)$')



ax.set_title('Some Sample Data with Error Bars')
ax.set_xlabel('x')
ax.set_ylabel('y')

# Here is the text we want to include...
textfit = '$f(x) = A + Bx$ \n' 
textfit += '$A = {:.2f} \\pm {:.2f}$ \n'.format(pf[0],pferr[0]) 
textfit +='$B = {:.2f} \\pm {:.2f}$ \n'.format(pf[1],pferr[1]) 
textfit += '$\\chi^2= {:.1f}$ \n'.format(chisq) 
textfit += '$N = {}$ (dof) \n'.format(dof) 
textfit += '$\\chi^2/N = {:.2f}$'.format(chisq/dof) 

#... and below is where we actually place it on the plot
ax.text(0.05, 0.95, textfit, transform=ax.transAxes , fontsize=12,verticalalignment='top')

ax.set_xlim([x.min()-0.5, x.max()+0.5])
  # x.min() is equal to the smallest x value in the entire array, x.max() is equal to the largest
  # Together this ensures that the axes always scale to be just slightly wider than the data.
ax.legend(loc='lower right')
#plt.savefig('Example2_Figure1.png',dpi=300)
plt.show()

if using_colab:
  files.download('Example2_Figure1.png') 





fig = plt.figure(figsize = (10,8))
ax =fig.add_subplot(2,1,1)
ay =fig.add_subplot(2,1,2)
#fig,(ax,ay) = plt.subplots(nrows=2, ncols=1)
#the above is an equivalent way of doing the same thing

ax.errorbar(x, y, dy, fmt='k.', label = 'Data')
    # A basic scatter plot of some data in red
X = np.linspace(x.min(), x.max(), 500)
ax.plot(X, linear(pf, X), 'r-', label = 'Linear Fit: $f(x)$')

# Here is the text we want to include...
textfit = '$f(x) = A + Bx$ \n' 
textfit += '$A = {:.2f} \\pm {:.2f}$ \n'.format(pf[0],pferr[0]) 
textfit +='$B = {:.2f} \\pm {:.2f}$ \n'.format(pf[1],pferr[1]) 
textfit += '$\\chi^2= {:.1f}$ \n'.format(chisq) 
textfit += '$N = {}$ (dof) \n'.format(dof) 
textfit += '$\\chi^2/N = {:.2f}$'.format(chisq/dof) 

#... and below is where we actually place it on the plot
ax.text(0.2, 0.95, textfit, transform=ax.transAxes , fontsize=12,verticalalignment='top')



ay.plot(x,residual(pf,linear, x, y, dy),'r.',label='Linear Fit Residuals')
    # A plot of just the magnitudes of the errors
ax.legend()
ay.legend()
ay.axhline(0)
ax.set_title("Data")
ay.set_title("Errors")
fig.tight_layout()





def quadratic(p, xvar):
    return p[0] + p[1] * xvar + p[2] * (xvar ** 2)


fig, ax = plt.subplots()
test_x = np.linspace(-2,2,100)

quad_guess = [0,0,1]

ax.plot(test_x,quadratic(quad_guess,test_x))


print("Quadratic Fit")
my_pf, my_pferr, my_chisq, my_dof = data_fit([1, 1, 1], quadratic, x, y, dy)


fig,(ax,ay) = plt.subplots(2, 1, figsize=(10, 8))
#the above is an equivalent way of doing the same thing

ax.errorbar(x, y, dy, fmt='k.', label = 'Data')
    # A basic scatter plot of some data in red
X = np.linspace(x.min(), x.max(), 500)
ax.plot(X, quadratic(my_pf, X), 'r-', label = 'Linear Fit: $f(x)$')

# Here is the text we want to include...
textfit = '$f(x) = A + Bx$ \n' 
textfit += '$A = {:.2f} \\pm {:.2f}$ \n'.format(my_pf[0], my_pferr[0]) 
textfit +='$B = {:.2f} \\pm {:.2f}$ \n'.format(my_pf[1], my_pferr[1]) 
textfit += '$\\chi^2= {:.1f}$ \n'.format(my_chisq) 
textfit += '$N = {}$ (dof) \n'.format(my_dof) 
textfit += '$\\chi^2/N = {:.2f}$'.format(my_chisq/my_dof) 

#... and below is where we actually place it on the plot
ax.text(0.2, 0.95, textfit, transform=ax.transAxes , fontsize=12,verticalalignment='top')

ay.plot(x,residual(my_pf, quadratic, x, y, dy),'r.',label='Linear Fit Residuals')
    # A plot of just the magnitudes of the errors
ax.legend()
ay.legend()
ay.axhline(0)
ax.set_title("Data")
ay.set_title("Errors")
fig.tight_layout()

plt.savefig('task5.pdf')



